{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision==0.17.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: torchaudio==2.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2.2.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (10.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.2.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyYAML\n",
      "Successfully installed PyYAML-6.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Print the version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p01_df = pd.read_csv('data/p01_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file and return the signal as a tensor and the sample rate\n",
    "signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1637, -0.1676, -0.1750,  ...,  0.0085,  0.0076,  0.0070],\n",
       "        [-0.1284, -0.1376, -0.1444,  ...,  0.0056,  0.0053,  0.0050]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the audio files is:  14.057006802721089 seconds\n"
     ]
    }
   ],
   "source": [
    "durations = []\n",
    "\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    durations.append(signal.size(1) / sample_rate)\n",
    "    \n",
    "# Find the maximum length in milliseconds of the audio files\n",
    "max_length = max(durations)\n",
    "\n",
    "print(\"The maximum length of the audio files is: \", max_length, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mono audio files: 314\n",
      "Number of stereo audio files: 1394\n"
     ]
    }
   ],
   "source": [
    "# Assuming p01_df['Filename'] contains the filenames of the audio files\n",
    "audio_dir = 'data/wav/'  # Replace with the path to your audio directory\n",
    "\n",
    "# Initialize counters for mono and stereo files\n",
    "mono_count = 0\n",
    "stereo_count = 0\n",
    "\n",
    "# Iterate over each file in the DataFrame\n",
    "for filename in p01_df['Filename']:\n",
    "    filepath = os.path.join(audio_dir, filename)\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = torchaudio.load(filepath)\n",
    "\n",
    "    # Check the number of channels\n",
    "    if signal.shape[0] == 1:\n",
    "        mono_count += 1\n",
    "    elif signal.shape[0] == 2:\n",
    "        stereo_count += 1\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of mono audio files: {mono_count}\")\n",
    "print(f\"Number of stereo audio files: {stereo_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mono(signal):\n",
    "    if signal.shape[0] == 2:  # If the signal has 2 channels (stereo)\n",
    "        signal = signal.mean(dim=0, keepdim=True)  # Convert to mono by averaging the channels\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200126_2142_00-13-04.06--00-13-04.324.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[-0.14604187 -0.15263367 -0.15974426 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200126_2142_00-06-41.54--00-06-42.47.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.08834839  0.09138489  0.09321594 ... -0.12...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200126_2142_00-11-35.94--00-11-37.08.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[0.0358429  0.02403259 0.01158142 ... 0.245162...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200126_2142_00-12-11.66--00-12-15.31.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.00675964 -0.00045776 -0.01092529 ...  0.09...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200126_2142_00-00-24.55--00-00-24.95.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.02839661  0.02764893  0.0249939  ... -0.29...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Filename Participant         Label  \\\n",
       "0  200126_2142_00-13-04.06--00-13-04.324.wav         P01  dysregulated   \n",
       "1   200126_2142_00-06-41.54--00-06-42.47.wav         P01  dysregulated   \n",
       "2   200126_2142_00-11-35.94--00-11-37.08.wav         P01  dysregulated   \n",
       "3   200126_2142_00-12-11.66--00-12-15.31.wav         P01  dysregulated   \n",
       "4   200126_2142_00-00-24.55--00-00-24.95.wav         P01  dysregulated   \n",
       "\n",
       "                                               Audio  delighted  dysregulated  \\\n",
       "0  [-0.14604187 -0.15263367 -0.15974426 ...  0.00...      False          True   \n",
       "1  [ 0.08834839  0.09138489  0.09321594 ... -0.12...      False          True   \n",
       "2  [0.0358429  0.02403259 0.01158142 ... 0.245162...      False          True   \n",
       "3  [ 0.00675964 -0.00045776 -0.01092529 ...  0.09...      False          True   \n",
       "4  [ 0.02839661  0.02764893  0.0249939  ... -0.29...      False          True   \n",
       "\n",
       "   frustrated  request  selftalk  social  \n",
       "0       False    False     False   False  \n",
       "1       False    False     False   False  \n",
       "2       False    False     False   False  \n",
       "3       False    False     False   False  \n",
       "4       False    False     False   False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p01_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length in samples\n",
    "max_length_samples = int(max_length * sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, define a function to pad the signal to the maximum length\n",
    "\n",
    "def pad_signal(signal, max_length_samples):\n",
    "    if signal.shape[1] < max_length_samples:\n",
    "        pad_begin_len = random.randint(0, max_length_samples - signal.shape[1]) # Begin padding length can be anything between 0 and the difference between the max length and the signal length\n",
    "        pad_end_len = max_length_samples - signal.shape[1] - pad_begin_len  # End padding length is the difference between the max length and the sum of the signal length and the begin padding length\n",
    "\n",
    "        # Pad with zeros\n",
    "        pad_begin = torch.zeros(signal.shape[0], pad_begin_len)\n",
    "        pad_end = torch.zeros(signal.shape[0], pad_end_len)\n",
    "\n",
    "        signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to shift the signal in time by a random amount between -0.5 and 0.5 seconds\n",
    "\n",
    "def shift_signal(signal, sample_rate):\n",
    "    shift_amount = random.uniform(-0.5, 0.5)\n",
    "    shift_samples = int(shift_amount * sample_rate) # Convert the shift amount to samples\n",
    "    \n",
    "    if shift_samples > 0:\n",
    "        # Shift the signal to the right\n",
    "        signal = torch.cat((torch.zeros(signal.shape[0], shift_samples), signal[:, :-shift_samples]), 1)\n",
    "    else:\n",
    "        # Shift the signal to the left\n",
    "        signal = torch.cat((signal[:, -shift_samples:], torch.zeros(signal.shape[0], -shift_samples)), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate mel spectrograms from the audio files\n",
    "\n",
    "def generate_mel_spectrogram(signal, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels, f_min=fmin, f_max=fmax, n_fft=n_fft)(signal)\n",
    "    mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
    "    \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do time and frequency masking on the mel spectrogram\n",
    "\n",
    "def mask_mel_spectrogram(mel_spectrogram, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    n_mel_channels = mel_spectrogram.shape[1] # Number of mel channels\n",
    "    n_mel_frames = mel_spectrogram.shape[2] # Number of mel frames\n",
    "    \n",
    "    max_mask_size_freq = int(max_mask_pct * n_mel_channels) # Maximum size of the frequency mask\n",
    "    max_mask_size_time = int(max_mask_pct * n_mel_frames) # Maximum size of the time mask\n",
    "    \n",
    "    for _ in range(n_freq_masks):\n",
    "        mask_size_freq = random.randint(0, max_mask_size_freq) # Random size of the frequency mask\n",
    "        mask_start_freq = random.randint(0, n_mel_channels - mask_size_freq) # Random start of the frequency mask\n",
    "        mel_spectrogram[:, mask_start_freq:mask_start_freq + mask_size_freq, :] = 0 \n",
    "        \n",
    "    for _ in range(n_time_masks):\n",
    "        mask_size_time = random.randint(0, max_mask_size_time) # Random size of the time mask\n",
    "        mask_start_time = random.randint(0, n_mel_frames - mask_size_time) # Random start of the time mask\n",
    "        mel_spectrogram[:, :, mask_start_time:mask_start_time + mask_size_time] = 0\n",
    "        \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to augment the audio files\n",
    "\n",
    "def augment_audio_files(audio_files, max_length_samples, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    mel_spectrograms = []\n",
    "    augmented_audio_files = [] # To store the augmented audio files\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        signal = torch.tensor(audio_files[i])\n",
    "        signal = convert_to_mono(signal)\n",
    "        \n",
    "        # Check if conversion to mono was successful\n",
    "        if signal.shape[0] != 1:\n",
    "            print(f\"Warning: Signal {i} is not mono after conversion. Shape: {signal.shape}\")\n",
    "\n",
    "        signal = pad_signal(signal, max_length_samples)\n",
    "        signal = shift_signal(signal, sample_rate)\n",
    "        \n",
    "        augmented_audio_files.append(signal.numpy()) # Append the augmented audio file\n",
    "        \n",
    "        mel_spectrogram = generate_mel_spectrogram(signal, sample_rate, n_mels, fmin, fmax, n_fft)\n",
    "        mel_spectrogram = mask_mel_spectrogram(mel_spectrogram, max_mask_pct, n_freq_masks, n_time_masks)\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "        \n",
    "    return mel_spectrograms, augmented_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4336/2902082806.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  signal = torch.tensor(audio_files[i])\n"
     ]
    }
   ],
   "source": [
    "# Augment the audio files\n",
    "\n",
    "audio_files = [] # List to store the audio files because apprently, csv files don't store the audio files as tensors\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    audio_files.append(signal)\n",
    "sample_rate = librosa.load('data/wav/' + p01_df['Filename'].iloc[0], sr=None)[1]\n",
    "mel_spectrograms, augmented_signals = augment_audio_files(audio_files, max_length_samples, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 80:20 train and test split for the dataset for participant 1 using the augmented audio files and mel spectrograms\n",
    "\n",
    "# Add the mel spectrograms and augmented audio files to the dataframe\n",
    "p01_df['Mel Spectrogram'] = mel_spectrograms\n",
    "p01_df['Augmented Audio'] = augmented_signals\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "# p01_df.to_csv('data/p01_df_augmented.csv', index=False) # Index is set to False to avoid saving the index column\n",
    "\n",
    "# Make the train and test split for the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(p01_df[['Augmented Audio', 'Mel Spectrogram']], p01_df.drop(['Augmented Audio', 'Mel Spectrogram'], axis=1), test_size=0.2, random_state=42, shuffle=True, stratify=p01_df['Label'])\n",
    "# Stratify the split based on the labels and not the one-hot encoded labels as the one-hot encoded labels are not present in the dataframe anymore and drop the augmented audio and mel spectrogram columns from the x dataframes as they are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1366, 2), (342, 2), (1366, 10), (342, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Augmented Audio</th>\n",
       "      <th>Mel Spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Augmented Audio  \\\n",
       "1186  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1059  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "866   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "990   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                 ...   \n",
       "1677  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1707  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1194  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1693  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "608   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        Mel Spectrogram  \n",
       "1186  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1626  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1059  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "866   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "990   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "...                                                 ...  \n",
       "1677  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1707  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1194  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1693  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "608   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "\n",
       "[342 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>200329_1113_00-13-38.86--00-13-39.94.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.0186615   0.00944519  0.0038147  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>200309_2035_00-05-10.63--00-05-11.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-0.00149536 -0.00054932 -0.00024414 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>200229_2244_00-05-22.03--00-05-23.17.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.03457642 -0.03807068 -0.04052734 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>200306_2024_00-03-20.26--00-03-20.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.01483154  0.01954651  0.02474976 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>200306_2024_00-17-31.11--00-17-31.68.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.11328125  0.10844421  0.1025238  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>200815_2140_00-00-15.88--00-00-18.55.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00408936  0.0065918   0.01071167 ... -0.03...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>200307_1826_00-05-53.14--00-05-53.71.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00387573  0.0035553   0.00352478 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>200306_2024_00-08-21.67--00-08-24.98.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.01591492 -0.01504517 -0.01268005 ... -0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>200529_1029_00-01-10.01--00-01-11.35.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>210324_2036_00-06-12.32--00-06-13.43.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>social</td>\n",
       "      <td>[-0.00427246 -0.00418091 -0.00411987 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Filename Participant      Label  \\\n",
       "1186  200329_1113_00-13-38.86--00-13-39.94.wav         P01   selftalk   \n",
       "1626  200309_2035_00-05-10.63--00-05-11.91.wav         P01  delighted   \n",
       "1059  200229_2244_00-05-22.03--00-05-23.17.wav         P01   selftalk   \n",
       "866   200306_2024_00-03-20.26--00-03-20.91.wav         P01   selftalk   \n",
       "990   200306_2024_00-17-31.11--00-17-31.68.wav         P01   selftalk   \n",
       "...                                        ...         ...        ...   \n",
       "1677  200815_2140_00-00-15.88--00-00-18.55.wav         P01  delighted   \n",
       "1707  200307_1826_00-05-53.14--00-05-53.71.wav         P01  delighted   \n",
       "1194  200306_2024_00-08-21.67--00-08-24.98.wav         P01   selftalk   \n",
       "1693  200529_1029_00-01-10.01--00-01-11.35.wav         P01  delighted   \n",
       "608   210324_2036_00-06-12.32--00-06-13.43.wav         P01     social   \n",
       "\n",
       "                                                  Audio  delighted  \\\n",
       "1186  [ 0.0186615   0.00944519  0.0038147  ... -0.01...      False   \n",
       "1626  [-0.00149536 -0.00054932 -0.00024414 ... -0.00...       True   \n",
       "1059  [-0.03457642 -0.03807068 -0.04052734 ...  0.00...      False   \n",
       "866   [ 0.01483154  0.01954651  0.02474976 ...  0.00...      False   \n",
       "990   [ 0.11328125  0.10844421  0.1025238  ... -0.01...      False   \n",
       "...                                                 ...        ...   \n",
       "1677  [ 0.00408936  0.0065918   0.01071167 ... -0.03...       True   \n",
       "1707  [ 0.00387573  0.0035553   0.00352478 ... -0.00...       True   \n",
       "1194  [-0.01591492 -0.01504517 -0.01268005 ... -0.00...      False   \n",
       "1693  [-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...       True   \n",
       "608   [-0.00427246 -0.00418091 -0.00411987 ...  0.00...      False   \n",
       "\n",
       "      dysregulated  frustrated  request  selftalk  social  \n",
       "1186         False       False    False      True   False  \n",
       "1626         False       False    False     False   False  \n",
       "1059         False       False    False      True   False  \n",
       "866          False       False    False      True   False  \n",
       "990          False       False    False      True   False  \n",
       "...            ...         ...      ...       ...     ...  \n",
       "1677         False       False    False     False   False  \n",
       "1707         False       False    False     False   False  \n",
       "1194         False       False    False      True   False  \n",
       "1693         False       False    False     False   False  \n",
       "608          False       False    False     False    True  \n",
       "\n",
       "[342 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename        object\n",
      "Participant     object\n",
      "Label           object\n",
      "Audio           object\n",
      "delighted         bool\n",
      "dysregulated      bool\n",
      "frustrated        bool\n",
      "request           bool\n",
      "selftalk          bool\n",
      "social            bool\n",
      "dtype: object\n",
      "                                        Filename Participant         Label  \\\n",
      "348     200124_1828_00-09-14.62--00-09-15.67.wav         P01  dysregulated   \n",
      "85    200229_2156_00-08-01.665--00-08-03.284.wav         P01    frustrated   \n",
      "1526    200309_2035_00-01-53.76--00-01-54.45.wav         P01     delighted   \n",
      "393    200124_1828_00-01-01.464--00-01-02.72.wav         P01  dysregulated   \n",
      "1080     200306_2024_00-02-28.21--00-02-29.9.wav         P01      selftalk   \n",
      "\n",
      "                                                  Audio  delighted  \\\n",
      "348   [-0.05300903 -0.06085205 -0.06741333 ... -0.22...      False   \n",
      "85    [-0.0071106  -0.00773621 -0.00802612 ... -0.00...      False   \n",
      "1526  [-5.6457520e-03 -5.6152344e-03 -5.5236816e-03 ...       True   \n",
      "393   [ 0.13546753  0.18415833  0.2346344  ... -0.11...      False   \n",
      "1080  [-0.00106812 -0.00361633 -0.00590515 ... -0.00...      False   \n",
      "\n",
      "      dysregulated  frustrated  request  selftalk  social  \n",
      "348           True       False    False     False   False  \n",
      "85           False        True    False     False   False  \n",
      "1526         False       False    False     False   False  \n",
      "393           True       False    False     False   False  \n",
      "1080         False       False    False      True   False  \n"
     ]
    }
   ],
   "source": [
    "print(y_train.dtypes)  # Check the data types of the columns in y_train\n",
    "print(y_train.head())  # Display the first few rows of y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the index of the label\n",
    "def get_label_index(row):\n",
    "    labels = ['delighted', 'dysregulated', 'frustrated', 'request', 'selftalk', 'social']\n",
    "    for idx, label in enumerate(labels):\n",
    "        if row[label]:  # If the label is True, return its index\n",
    "            return idx\n",
    "    return -1  # If no label is True, return an invalid index (this shouldn't happen in a clean dataset)\n",
    "\n",
    "# Apply the function to create a label index for each row\n",
    "y_train['Label_Index'] = y_train.apply(get_label_index, axis=1)\n",
    "y_test['Label_Index'] = y_test.apply(get_label_index, axis=1)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "train_labels = torch.tensor(y_train['Label_Index'].values).long()\n",
    "test_labels = torch.tensor(y_test['Label_Index'].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "    # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "    train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "    test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "\n",
    "    # Convert labels to single label index\n",
    "    y_train['Label_Index'] = y_train.apply(get_label_index, axis=1)\n",
    "    y_test['Label_Index'] = y_test.apply(get_label_index, axis=1)\n",
    "\n",
    "    train_labels = torch.tensor(y_train['Label_Index'].values).long()\n",
    "    test_labels = torch.tensor(y_test['Label_Index'].values).long()\n",
    "\n",
    "    # Create TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4336/3990731225.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4336/3990731225.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.06326 | Train Acc: 0.26940 | Val Loss: 0.05362 | Val Acc: 0.29825\n",
      "Epoch: 1 | Train Loss: 0.05177 | Train Acc: 0.31552 | Val Loss: 0.05227 | Val Acc: 0.32164\n",
      "Epoch: 2 | Train Loss: 0.05019 | Train Acc: 0.34773 | Val Loss: 0.05066 | Val Acc: 0.33041\n",
      "Epoch: 3 | Train Loss: 0.04905 | Train Acc: 0.36969 | Val Loss: 0.05113 | Val Acc: 0.32456\n",
      "Epoch: 4 | Train Loss: 0.04795 | Train Acc: 0.37555 | Val Loss: 0.04836 | Val Acc: 0.35965\n",
      "Epoch: 5 | Train Loss: 0.04621 | Train Acc: 0.38873 | Val Loss: 0.04685 | Val Acc: 0.38596\n",
      "Epoch: 6 | Train Loss: 0.04356 | Train Acc: 0.43485 | Val Loss: 0.04591 | Val Acc: 0.40058\n",
      "Epoch: 7 | Train Loss: 0.04313 | Train Acc: 0.43558 | Val Loss: 0.04681 | Val Acc: 0.38596\n",
      "Epoch: 8 | Train Loss: 0.04102 | Train Acc: 0.48097 | Val Loss: 0.04602 | Val Acc: 0.42105\n",
      "Epoch: 9 | Train Loss: 0.03913 | Train Acc: 0.48902 | Val Loss: 0.04389 | Val Acc: 0.42690\n",
      "Epoch: 10 | Train Loss: 0.03786 | Train Acc: 0.52196 | Val Loss: 0.04237 | Val Acc: 0.44444\n",
      "Epoch: 11 | Train Loss: 0.03518 | Train Acc: 0.54246 | Val Loss: 0.04132 | Val Acc: 0.49708\n",
      "Epoch: 12 | Train Loss: 0.03334 | Train Acc: 0.58126 | Val Loss: 0.04477 | Val Acc: 0.50000\n",
      "Epoch: 13 | Train Loss: 0.03056 | Train Acc: 0.61786 | Val Loss: 0.04199 | Val Acc: 0.47661\n",
      "Epoch: 14 | Train Loss: 0.02683 | Train Acc: 0.66911 | Val Loss: 0.04226 | Val Acc: 0.52047\n",
      "Epoch: 15 | Train Loss: 0.02533 | Train Acc: 0.70205 | Val Loss: 0.04748 | Val Acc: 0.52047\n",
      "Epoch: 16 | Train Loss: 0.02342 | Train Acc: 0.71449 | Val Loss: 0.04811 | Val Acc: 0.49123\n",
      "Epoch: 17 | Train Loss: 0.02118 | Train Acc: 0.74963 | Val Loss: 0.05741 | Val Acc: 0.47661\n",
      "Epoch: 18 | Train Loss: 0.01820 | Train Acc: 0.78331 | Val Loss: 0.04598 | Val Acc: 0.50877\n",
      "Epoch: 19 | Train Loss: 0.01357 | Train Acc: 0.86164 | Val Loss: 0.04850 | Val Acc: 0.51170\n",
      "Epoch: 20 | Train Loss: 0.01242 | Train Acc: 0.86457 | Val Loss: 0.05076 | Val Acc: 0.52339\n",
      "Epoch: 21 | Train Loss: 0.01114 | Train Acc: 0.88214 | Val Loss: 0.05200 | Val Acc: 0.52924\n",
      "Epoch: 22 | Train Loss: 0.01020 | Train Acc: 0.89898 | Val Loss: 0.05493 | Val Acc: 0.52047\n",
      "Epoch: 23 | Train Loss: 0.00961 | Train Acc: 0.90703 | Val Loss: 0.05720 | Val Acc: 0.52632\n",
      "Epoch: 24 | Train Loss: 0.00884 | Train Acc: 0.90337 | Val Loss: 0.05739 | Val Acc: 0.52924\n",
      "Epoch: 25 | Train Loss: 0.00887 | Train Acc: 0.91874 | Val Loss: 0.05731 | Val Acc: 0.53216\n",
      "Epoch: 26 | Train Loss: 0.00859 | Train Acc: 0.92167 | Val Loss: 0.05747 | Val Acc: 0.52924\n",
      "Epoch: 27 | Train Loss: 0.00879 | Train Acc: 0.91215 | Val Loss: 0.05751 | Val Acc: 0.53216\n",
      "Epoch: 28 | Train Loss: 0.00871 | Train Acc: 0.91508 | Val Loss: 0.05790 | Val Acc: 0.52339\n",
      "Epoch: 29 | Train Loss: 0.00855 | Train Acc: 0.92387 | Val Loss: 0.05796 | Val Acc: 0.52339\n",
      "Epoch: 30 | Train Loss: 0.00848 | Train Acc: 0.91801 | Val Loss: 0.05798 | Val Acc: 0.52339\n",
      "Epoch: 31 | Train Loss: 0.00834 | Train Acc: 0.92387 | Val Loss: 0.05803 | Val Acc: 0.52339\n",
      "Epoch: 32 | Train Loss: 0.00835 | Train Acc: 0.92020 | Val Loss: 0.05806 | Val Acc: 0.52339\n",
      "Epoch: 33 | Train Loss: 0.00854 | Train Acc: 0.91654 | Val Loss: 0.05805 | Val Acc: 0.52632\n",
      "Epoch: 34 | Train Loss: 0.00849 | Train Acc: 0.91728 | Val Loss: 0.05806 | Val Acc: 0.52632\n",
      "Epoch: 35 | Train Loss: 0.00861 | Train Acc: 0.92167 | Val Loss: 0.05804 | Val Acc: 0.52924\n",
      "Epoch: 36 | Train Loss: 0.00854 | Train Acc: 0.91801 | Val Loss: 0.05805 | Val Acc: 0.52924\n",
      "Epoch: 37 | Train Loss: 0.00844 | Train Acc: 0.92606 | Val Loss: 0.05805 | Val Acc: 0.52924\n",
      "Epoch: 38 | Train Loss: 0.00831 | Train Acc: 0.92020 | Val Loss: 0.05806 | Val Acc: 0.52924\n",
      "Epoch: 39 | Train Loss: 0.00843 | Train Acc: 0.91508 | Val Loss: 0.05807 | Val Acc: 0.52924\n",
      "Test Accuracy: 0.52924\n"
     ]
    }
   ],
   "source": [
    "class AudioClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Add global average pooling\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(256, 512)  # Adjust to the reduced size after GAP\n",
    "        self.fc2 = torch.nn.Linear(512, n_classes)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)  # Apply Global Average Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the function to train the model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device) # Move the data to the device\n",
    "\n",
    "            optimizer.zero_grad() # Zero the gradients thereby preventing them from accumulating\n",
    "\n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "            loss = criterion(y_pred, y) # Calculate the loss\n",
    "            loss.backward() # Backpropagate the loss\n",
    "            optimizer.step() # Update the weights\n",
    "\n",
    "            train_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "        train_loss /= len(train_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "        train_acc /= len(train_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "            for i, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "                loss = criterion(y_pred, y) # Calculate the loss\n",
    "\n",
    "                val_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "            val_loss /= len(val_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "            val_acc /= len(val_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "\n",
    "        print(\"Epoch: {} | Train Loss: {:.5f} | Train Acc: {:.5f} | Val Loss: {:.5f} | Val Acc: {:.5f}\".format(epoch, train_loss, train_acc, val_loss, val_acc)) # Print the epoch number, training loss, training accuracy, validation loss and validation accuracy\n",
    "\n",
    "        scheduler.step(val_loss) # Adjust the learning rate based on the validation loss\n",
    "\n",
    "# Define the function to test the model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode because certain layers like dropout behave differently in training and evaluation mode\n",
    "    test_acc = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"Test Accuracy: {:.5f}\".format(test_acc))\n",
    "\n",
    "# Define the function to create the dataloaders\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     train_dataset = torch.utils.data.TensorDataset(X_train['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_train.values).long())\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     val_dataset = torch.utils.data.TensorDataset(X_test['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_test.values).long())\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "#     train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "#     test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "\n",
    "#     # Convert one-hot encoded labels to class indices\n",
    "#     train_labels = torch.argmax(torch.tensor(y_train.values), dim=1).long()\n",
    "#     test_labels = torch.argmax(torch.tensor(y_test.values), dim=1).long()\n",
    "\n",
    "#     # Create TensorDataset\n",
    "#     train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "#     val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "\n",
    "#     # Create DataLoaders\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "\n",
    "n_classes = len(p01_df['Label'].unique())\n",
    "n_epochs = 40\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader, val_loader = create_dataloaders(X_train, X_test, y_train, y_test, batch_size)\n",
    "\n",
    "# Create the model, criterion, optimizer and scheduler\n",
    "\n",
    "model = AudioClassifier(n_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "test_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4336/3990731225.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
      "/tmp/ipykernel_4336/3990731225.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.06661 | Train Acc: 0.27306 | Val Loss: 0.05406 | Val Acc: 0.29532\n",
      "Epoch: 1 | Train Loss: 0.05213 | Train Acc: 0.28917 | Val Loss: 0.05239 | Val Acc: 0.34211\n",
      "Epoch: 2 | Train Loss: 0.05134 | Train Acc: 0.33821 | Val Loss: 0.05148 | Val Acc: 0.33041\n",
      "Epoch: 3 | Train Loss: 0.05049 | Train Acc: 0.34334 | Val Loss: 0.04940 | Val Acc: 0.36550\n",
      "Epoch: 4 | Train Loss: 0.04791 | Train Acc: 0.36896 | Val Loss: 0.04799 | Val Acc: 0.38596\n",
      "Epoch: 5 | Train Loss: 0.04693 | Train Acc: 0.38873 | Val Loss: 0.05043 | Val Acc: 0.35088\n",
      "Epoch: 6 | Train Loss: 0.04589 | Train Acc: 0.38141 | Val Loss: 0.04663 | Val Acc: 0.39766\n",
      "Epoch: 7 | Train Loss: 0.04338 | Train Acc: 0.43338 | Val Loss: 0.04548 | Val Acc: 0.40643\n",
      "Epoch: 8 | Train Loss: 0.04241 | Train Acc: 0.43265 | Val Loss: 0.04508 | Val Acc: 0.38596\n",
      "Epoch: 9 | Train Loss: 0.04071 | Train Acc: 0.44583 | Val Loss: 0.04439 | Val Acc: 0.41520\n",
      "Epoch: 10 | Train Loss: 0.03931 | Train Acc: 0.46633 | Val Loss: 0.04424 | Val Acc: 0.44444\n",
      "Epoch: 11 | Train Loss: 0.03703 | Train Acc: 0.49707 | Val Loss: 0.04291 | Val Acc: 0.46199\n",
      "Epoch: 12 | Train Loss: 0.03588 | Train Acc: 0.52562 | Val Loss: 0.04216 | Val Acc: 0.46491\n",
      "Epoch: 13 | Train Loss: 0.03347 | Train Acc: 0.55710 | Val Loss: 0.04534 | Val Acc: 0.46199\n",
      "Epoch: 14 | Train Loss: 0.03369 | Train Acc: 0.56735 | Val Loss: 0.05017 | Val Acc: 0.42690\n",
      "Epoch: 15 | Train Loss: 0.03244 | Train Acc: 0.58931 | Val Loss: 0.04606 | Val Acc: 0.44152\n",
      "Epoch: 16 | Train Loss: 0.03109 | Train Acc: 0.60249 | Val Loss: 0.04834 | Val Acc: 0.46784\n",
      "Epoch: 17 | Train Loss: 0.02753 | Train Acc: 0.65007 | Val Loss: 0.04683 | Val Acc: 0.49123\n",
      "Epoch: 18 | Train Loss: 0.02708 | Train Acc: 0.66398 | Val Loss: 0.05058 | Val Acc: 0.47368\n",
      "Epoch: 19 | Train Loss: 0.02177 | Train Acc: 0.72401 | Val Loss: 0.04507 | Val Acc: 0.50000\n",
      "Epoch: 20 | Train Loss: 0.02001 | Train Acc: 0.74451 | Val Loss: 0.04616 | Val Acc: 0.49415\n",
      "Epoch: 21 | Train Loss: 0.01869 | Train Acc: 0.77233 | Val Loss: 0.04720 | Val Acc: 0.49708\n",
      "Epoch: 22 | Train Loss: 0.01781 | Train Acc: 0.77965 | Val Loss: 0.04792 | Val Acc: 0.50000\n",
      "Epoch: 23 | Train Loss: 0.01714 | Train Acc: 0.78990 | Val Loss: 0.04952 | Val Acc: 0.50877\n",
      "Epoch: 24 | Train Loss: 0.01624 | Train Acc: 0.80381 | Val Loss: 0.05019 | Val Acc: 0.50000\n",
      "Epoch: 25 | Train Loss: 0.01546 | Train Acc: 0.81845 | Val Loss: 0.05060 | Val Acc: 0.51754\n",
      "Epoch: 26 | Train Loss: 0.01518 | Train Acc: 0.82064 | Val Loss: 0.05077 | Val Acc: 0.51462\n",
      "Epoch: 27 | Train Loss: 0.01526 | Train Acc: 0.82211 | Val Loss: 0.05084 | Val Acc: 0.51170\n",
      "Epoch: 28 | Train Loss: 0.01524 | Train Acc: 0.82284 | Val Loss: 0.05076 | Val Acc: 0.51462\n",
      "Epoch: 29 | Train Loss: 0.01507 | Train Acc: 0.81991 | Val Loss: 0.05088 | Val Acc: 0.51170\n",
      "Epoch: 30 | Train Loss: 0.01501 | Train Acc: 0.81698 | Val Loss: 0.05118 | Val Acc: 0.51170\n",
      "Epoch: 31 | Train Loss: 0.01495 | Train Acc: 0.81991 | Val Loss: 0.05121 | Val Acc: 0.51170\n",
      "Epoch: 32 | Train Loss: 0.01467 | Train Acc: 0.83089 | Val Loss: 0.05123 | Val Acc: 0.51170\n",
      "Epoch: 33 | Train Loss: 0.01469 | Train Acc: 0.84187 | Val Loss: 0.05127 | Val Acc: 0.51170\n",
      "Epoch: 34 | Train Loss: 0.01453 | Train Acc: 0.83163 | Val Loss: 0.05127 | Val Acc: 0.51170\n",
      "Epoch: 35 | Train Loss: 0.01471 | Train Acc: 0.82796 | Val Loss: 0.05128 | Val Acc: 0.51170\n",
      "Epoch: 36 | Train Loss: 0.01468 | Train Acc: 0.83163 | Val Loss: 0.05131 | Val Acc: 0.51170\n",
      "Epoch: 37 | Train Loss: 0.01453 | Train Acc: 0.82943 | Val Loss: 0.05131 | Val Acc: 0.51170\n",
      "Epoch: 38 | Train Loss: 0.01477 | Train Acc: 0.83089 | Val Loss: 0.05131 | Val Acc: 0.51170\n",
      "Epoch: 39 | Train Loss: 0.01466 | Train Acc: 0.82357 | Val Loss: 0.05131 | Val Acc: 0.51170\n",
      "Epoch: 40 | Train Loss: 0.01455 | Train Acc: 0.84041 | Val Loss: 0.05131 | Val Acc: 0.51170\n",
      "Epoch: 41 | Train Loss: 0.01457 | Train Acc: 0.83089 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 42 | Train Loss: 0.01452 | Train Acc: 0.83236 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 43 | Train Loss: 0.01474 | Train Acc: 0.82504 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 44 | Train Loss: 0.01452 | Train Acc: 0.82943 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 45 | Train Loss: 0.01487 | Train Acc: 0.82138 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 46 | Train Loss: 0.01470 | Train Acc: 0.82357 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 47 | Train Loss: 0.01471 | Train Acc: 0.82284 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 48 | Train Loss: 0.01495 | Train Acc: 0.81918 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 49 | Train Loss: 0.01495 | Train Acc: 0.82064 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 50 | Train Loss: 0.01473 | Train Acc: 0.83163 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 51 | Train Loss: 0.01457 | Train Acc: 0.82211 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 52 | Train Loss: 0.01487 | Train Acc: 0.83309 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 53 | Train Loss: 0.01453 | Train Acc: 0.82796 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 54 | Train Loss: 0.01453 | Train Acc: 0.82870 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 55 | Train Loss: 0.01469 | Train Acc: 0.82650 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 56 | Train Loss: 0.01462 | Train Acc: 0.82870 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 57 | Train Loss: 0.01489 | Train Acc: 0.82504 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 58 | Train Loss: 0.01454 | Train Acc: 0.83602 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 59 | Train Loss: 0.01475 | Train Acc: 0.82650 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 60 | Train Loss: 0.01488 | Train Acc: 0.82211 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 61 | Train Loss: 0.01480 | Train Acc: 0.82284 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 62 | Train Loss: 0.01473 | Train Acc: 0.82796 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 63 | Train Loss: 0.01442 | Train Acc: 0.83309 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 64 | Train Loss: 0.01469 | Train Acc: 0.82577 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 65 | Train Loss: 0.01465 | Train Acc: 0.82357 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 66 | Train Loss: 0.01453 | Train Acc: 0.83382 | Val Loss: 0.05132 | Val Acc: 0.51170\n",
      "Epoch: 67 | Train Loss: 0.01453 | Train Acc: 0.83236 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 68 | Train Loss: 0.01490 | Train Acc: 0.82504 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 69 | Train Loss: 0.01509 | Train Acc: 0.82430 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 70 | Train Loss: 0.01487 | Train Acc: 0.82577 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 71 | Train Loss: 0.01470 | Train Acc: 0.82504 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 72 | Train Loss: 0.01470 | Train Acc: 0.83309 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 73 | Train Loss: 0.01456 | Train Acc: 0.83089 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 74 | Train Loss: 0.01452 | Train Acc: 0.82430 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 75 | Train Loss: 0.01468 | Train Acc: 0.83602 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 76 | Train Loss: 0.01493 | Train Acc: 0.83089 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 77 | Train Loss: 0.01467 | Train Acc: 0.81918 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 78 | Train Loss: 0.01478 | Train Acc: 0.81918 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 79 | Train Loss: 0.01458 | Train Acc: 0.83163 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 80 | Train Loss: 0.01474 | Train Acc: 0.83529 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 81 | Train Loss: 0.01460 | Train Acc: 0.82796 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 82 | Train Loss: 0.01474 | Train Acc: 0.83163 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 83 | Train Loss: 0.01509 | Train Acc: 0.82211 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 84 | Train Loss: 0.01507 | Train Acc: 0.82577 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 85 | Train Loss: 0.01487 | Train Acc: 0.82430 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 86 | Train Loss: 0.01449 | Train Acc: 0.82943 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 87 | Train Loss: 0.01477 | Train Acc: 0.83748 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 88 | Train Loss: 0.01465 | Train Acc: 0.83089 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 89 | Train Loss: 0.01480 | Train Acc: 0.82504 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 90 | Train Loss: 0.01481 | Train Acc: 0.82211 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 91 | Train Loss: 0.01465 | Train Acc: 0.82138 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 92 | Train Loss: 0.01465 | Train Acc: 0.82650 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 93 | Train Loss: 0.01473 | Train Acc: 0.82650 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 94 | Train Loss: 0.01494 | Train Acc: 0.82211 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 95 | Train Loss: 0.01488 | Train Acc: 0.82211 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 96 | Train Loss: 0.01495 | Train Acc: 0.82430 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 97 | Train Loss: 0.01478 | Train Acc: 0.82504 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 98 | Train Loss: 0.01473 | Train Acc: 0.82796 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Epoch: 99 | Train Loss: 0.01456 | Train Acc: 0.82723 | Val Loss: 0.05133 | Val Acc: 0.51170\n",
      "Test Accuracy: 0.51170\n"
     ]
    }
   ],
   "source": [
    "class AudioClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Add global average pooling\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(256, 512)  # Adjust to the reduced size after GAP\n",
    "        self.fc2 = torch.nn.Linear(512, n_classes)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)  # Apply Global Average Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the function to train the model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device) # Move the data to the device\n",
    "\n",
    "            optimizer.zero_grad() # Zero the gradients thereby preventing them from accumulating\n",
    "\n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "            loss = criterion(y_pred, y) # Calculate the loss\n",
    "            loss.backward() # Backpropagate the loss\n",
    "            optimizer.step() # Update the weights\n",
    "\n",
    "            train_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "        train_loss /= len(train_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "        train_acc /= len(train_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "            for i, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "                loss = criterion(y_pred, y) # Calculate the loss\n",
    "\n",
    "                val_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "            val_loss /= len(val_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "            val_acc /= len(val_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "\n",
    "        print(\"Epoch: {} | Train Loss: {:.5f} | Train Acc: {:.5f} | Val Loss: {:.5f} | Val Acc: {:.5f}\".format(epoch, train_loss, train_acc, val_loss, val_acc)) # Print the epoch number, training loss, training accuracy, validation loss and validation accuracy\n",
    "\n",
    "        scheduler.step(val_loss) # Adjust the learning rate based on the validation loss\n",
    "\n",
    "# Define the function to test the model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode because certain layers like dropout behave differently in training and evaluation mode\n",
    "    test_acc = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "\n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"Test Accuracy: {:.5f}\".format(test_acc))\n",
    "\n",
    "# Define the function to create the dataloaders\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     train_dataset = torch.utils.data.TensorDataset(X_train['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_train.values).long())\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     val_dataset = torch.utils.data.TensorDataset(X_test['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_test.values).long())\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "#     train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "#     test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "\n",
    "#     # Convert one-hot encoded labels to class indices\n",
    "#     train_labels = torch.argmax(torch.tensor(y_train.values), dim=1).long()\n",
    "#     test_labels = torch.argmax(torch.tensor(y_test.values), dim=1).long()\n",
    "\n",
    "#     # Create TensorDataset\n",
    "#     train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "#     val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "\n",
    "#     # Create DataLoaders\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "\n",
    "n_classes = len(p01_df['Label'].unique())\n",
    "n_epochs = 100 #\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader, val_loader = create_dataloaders(X_train, X_test, y_train, y_test, batch_size)\n",
    "\n",
    "# Create the model, criterion, optimizer and scheduler\n",
    "\n",
    "model = AudioClassifier(n_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "test_model(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
