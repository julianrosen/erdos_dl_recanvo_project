{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision==0.17.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: torchaudio==2.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2.2.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (10.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.2.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyYAML\n",
      "Successfully installed PyYAML-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchaudio' has no attribute 'lib' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     AudioMetaData,\n\u001b[1;32m      5\u001b[0m     get_audio_backend,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     set_audio_backend,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     compliance,\n\u001b[1;32m     15\u001b[0m     datasets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     utils,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/_extension/__init__.py:42\u001b[0m\n\u001b[1;32m     38\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43m_check_cuda_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m _IS_RIR_AVAILABLE \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_torchaudio\u001b[38;5;241m.\u001b[39mis_rir_available()\n\u001b[1;32m     44\u001b[0m _IS_ALIGN_AVAILABLE \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_torchaudio\u001b[38;5;241m.\u001b[39mis_align_available()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/_extension/utils.py:168\u001b[0m, in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_cuda_version\u001b[39m():\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241m.\u001b[39m_torchaudio\u001b[38;5;241m.\u001b[39mcuda_version()\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         version_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(version)\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchaudio' has no attribute 'lib' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Print the version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p01_df = pd.read_csv('data/p01_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file and return the signal as a tensor and the sample rate\n",
    "signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1637, -0.1676, -0.1750,  ...,  0.0085,  0.0076,  0.0070],\n",
       "        [-0.1284, -0.1376, -0.1444,  ...,  0.0056,  0.0053,  0.0050]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the audio files is:  14.057006802721089 seconds\n"
     ]
    }
   ],
   "source": [
    "durations = []\n",
    "\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    durations.append(signal.size(1) / sample_rate)\n",
    "    \n",
    "# Find the maximum length in milliseconds of the audio files\n",
    "max_length = max(durations)\n",
    "\n",
    "print(\"The maximum length of the audio files is: \", max_length, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mono audio files: 314\n",
      "Number of stereo audio files: 1394\n"
     ]
    }
   ],
   "source": [
    "# Assuming p01_df['Filename'] contains the filenames of the audio files\n",
    "audio_dir = 'data/wav/'  # Replace with the path to your audio directory\n",
    "\n",
    "# Initialize counters for mono and stereo files\n",
    "mono_count = 0\n",
    "stereo_count = 0\n",
    "\n",
    "# Iterate over each file in the DataFrame\n",
    "for filename in p01_df['Filename']:\n",
    "    filepath = os.path.join(audio_dir, filename)\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = torchaudio.load(filepath)\n",
    "\n",
    "    # Check the number of channels\n",
    "    if signal.shape[0] == 1:\n",
    "        mono_count += 1\n",
    "    elif signal.shape[0] == 2:\n",
    "        stereo_count += 1\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of mono audio files: {mono_count}\")\n",
    "print(f\"Number of stereo audio files: {stereo_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mono(signal):\n",
    "    if signal.shape[0] == 2:  # If the signal has 2 channels (stereo)\n",
    "        signal = signal.mean(dim=0, keepdim=True)  # Convert to mono by averaging the channels\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200126_2142_00-13-04.06--00-13-04.324.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[-0.14604187 -0.15263367 -0.15974426 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200126_2142_00-06-41.54--00-06-42.47.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.08834839  0.09138489  0.09321594 ... -0.12...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200126_2142_00-11-35.94--00-11-37.08.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[0.0358429  0.02403259 0.01158142 ... 0.245162...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200126_2142_00-12-11.66--00-12-15.31.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.00675964 -0.00045776 -0.01092529 ...  0.09...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200126_2142_00-00-24.55--00-00-24.95.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.02839661  0.02764893  0.0249939  ... -0.29...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Filename Participant         Label  \\\n",
       "0  200126_2142_00-13-04.06--00-13-04.324.wav         P01  dysregulated   \n",
       "1   200126_2142_00-06-41.54--00-06-42.47.wav         P01  dysregulated   \n",
       "2   200126_2142_00-11-35.94--00-11-37.08.wav         P01  dysregulated   \n",
       "3   200126_2142_00-12-11.66--00-12-15.31.wav         P01  dysregulated   \n",
       "4   200126_2142_00-00-24.55--00-00-24.95.wav         P01  dysregulated   \n",
       "\n",
       "                                               Audio  delighted  dysregulated  \\\n",
       "0  [-0.14604187 -0.15263367 -0.15974426 ...  0.00...      False          True   \n",
       "1  [ 0.08834839  0.09138489  0.09321594 ... -0.12...      False          True   \n",
       "2  [0.0358429  0.02403259 0.01158142 ... 0.245162...      False          True   \n",
       "3  [ 0.00675964 -0.00045776 -0.01092529 ...  0.09...      False          True   \n",
       "4  [ 0.02839661  0.02764893  0.0249939  ... -0.29...      False          True   \n",
       "\n",
       "   frustrated  request  selftalk  social  \n",
       "0       False    False     False   False  \n",
       "1       False    False     False   False  \n",
       "2       False    False     False   False  \n",
       "3       False    False     False   False  \n",
       "4       False    False     False   False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p01_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length in samples\n",
    "max_length_samples = int(max_length * sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, define a function to pad the signal to the maximum length\n",
    "\n",
    "def pad_signal(signal, max_length_samples):\n",
    "    if signal.shape[1] < max_length_samples:\n",
    "        pad_begin_len = random.randint(0, max_length_samples - signal.shape[1]) # Begin padding length can be anything between 0 and the difference between the max length and the signal length\n",
    "        pad_end_len = max_length_samples - signal.shape[1] - pad_begin_len  # End padding length is the difference between the max length and the sum of the signal length and the begin padding length\n",
    "\n",
    "        # Pad with zeros\n",
    "        pad_begin = torch.zeros(signal.shape[0], pad_begin_len)\n",
    "        pad_end = torch.zeros(signal.shape[0], pad_end_len)\n",
    "\n",
    "        signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to shift the signal in time by a random amount between -0.5 and 0.5 seconds\n",
    "\n",
    "def shift_signal(signal, sample_rate):\n",
    "    shift_amount = random.uniform(-0.5, 0.5)\n",
    "    shift_samples = int(shift_amount * sample_rate) # Convert the shift amount to samples\n",
    "    \n",
    "    if shift_samples > 0:\n",
    "        # Shift the signal to the right\n",
    "        signal = torch.cat((torch.zeros(signal.shape[0], shift_samples), signal[:, :-shift_samples]), 1)\n",
    "    else:\n",
    "        # Shift the signal to the left\n",
    "        signal = torch.cat((signal[:, -shift_samples:], torch.zeros(signal.shape[0], -shift_samples)), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate mel spectrograms from the audio files\n",
    "\n",
    "def generate_mel_spectrogram(signal, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels, f_min=fmin, f_max=fmax, n_fft=n_fft)(signal)\n",
    "    mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
    "    \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do time and frequency masking on the mel spectrogram\n",
    "\n",
    "def mask_mel_spectrogram(mel_spectrogram, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    n_mel_channels = mel_spectrogram.shape[1] # Number of mel channels\n",
    "    n_mel_frames = mel_spectrogram.shape[2] # Number of mel frames\n",
    "    \n",
    "    max_mask_size_freq = int(max_mask_pct * n_mel_channels) # Maximum size of the frequency mask\n",
    "    max_mask_size_time = int(max_mask_pct * n_mel_frames) # Maximum size of the time mask\n",
    "    \n",
    "    for _ in range(n_freq_masks):\n",
    "        mask_size_freq = random.randint(0, max_mask_size_freq) # Random size of the frequency mask\n",
    "        mask_start_freq = random.randint(0, n_mel_channels - mask_size_freq) # Random start of the frequency mask\n",
    "        mel_spectrogram[:, mask_start_freq:mask_start_freq + mask_size_freq, :] = 0 \n",
    "        \n",
    "    for _ in range(n_time_masks):\n",
    "        mask_size_time = random.randint(0, max_mask_size_time) # Random size of the time mask\n",
    "        mask_start_time = random.randint(0, n_mel_frames - mask_size_time) # Random start of the time mask\n",
    "        mel_spectrogram[:, :, mask_start_time:mask_start_time + mask_size_time] = 0\n",
    "        \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to augment the audio files\n",
    "\n",
    "def augment_audio_files(audio_files, max_length_samples, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    mel_spectrograms = []\n",
    "    augmented_audio_files = [] # To store the augmented audio files\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        signal = torch.tensor(audio_files[i])\n",
    "        signal = convert_to_mono(signal)\n",
    "        \n",
    "        # Check if conversion to mono was successful\n",
    "        if signal.shape[0] != 1:\n",
    "            print(f\"Warning: Signal {i} is not mono after conversion. Shape: {signal.shape}\")\n",
    "\n",
    "        signal = pad_signal(signal, max_length_samples)\n",
    "        signal = shift_signal(signal, sample_rate)\n",
    "        \n",
    "        augmented_audio_files.append(signal.numpy()) # Append the augmented audio file\n",
    "        \n",
    "        mel_spectrogram = generate_mel_spectrogram(signal, sample_rate, n_mels, fmin, fmax, n_fft)\n",
    "        mel_spectrogram = mask_mel_spectrogram(mel_spectrogram, max_mask_pct, n_freq_masks, n_time_masks)\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "        \n",
    "    return mel_spectrograms, augmented_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27919/2902082806.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  signal = torch.tensor(audio_files[i])\n"
     ]
    }
   ],
   "source": [
    "# Augment the audio files\n",
    "\n",
    "audio_files = [] # List to store the audio files because apprently, csv files don't store the audio files as tensors\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    audio_files.append(signal)\n",
    "sample_rate = librosa.load('data/wav/' + p01_df['Filename'].iloc[0], sr=None)[1]\n",
    "mel_spectrograms, augmented_signals = augment_audio_files(audio_files, max_length_samples, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 80:20 train and test split for the dataset for participant 1 using the augmented audio files and mel spectrograms\n",
    "\n",
    "# Add the mel spectrograms and augmented audio files to the dataframe\n",
    "p01_df['Mel Spectrogram'] = mel_spectrograms\n",
    "p01_df['Augmented Audio'] = augmented_signals\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "# p01_df.to_csv('data/p01_df_augmented.csv', index=False) # Index is set to False to avoid saving the index column\n",
    "\n",
    "# Make the train and test split for the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(p01_df[['Augmented Audio', 'Mel Spectrogram']], p01_df.drop(['Augmented Audio', 'Mel Spectrogram'], axis=1), test_size=0.2, random_state=42, shuffle=True, stratify=p01_df['Label'])\n",
    "# Stratify the split based on the labels and not the one-hot encoded labels as the one-hot encoded labels are not present in the dataframe anymore and drop the augmented audio and mel spectrogram columns from the x dataframes as they are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1366, 2), (342, 2), (1366, 10), (342, 10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Augmented Audio</th>\n",
       "      <th>Mel Spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>[[-0.008270264, -0.008117676, -0.007873535, -0...</td>\n",
       "      <td>[[[tensor(2.5152), tensor(0.0016), tensor(0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Augmented Audio  \\\n",
       "1186  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1059  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "866   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "990   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                 ...   \n",
       "1677  [[-0.008270264, -0.008117676, -0.007873535, -0...   \n",
       "1707  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1194  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1693  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "608   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        Mel Spectrogram  \n",
       "1186  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1626  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1059  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "866   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "990   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "...                                                 ...  \n",
       "1677  [[[tensor(2.5152), tensor(0.0016), tensor(0.00...  \n",
       "1707  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1194  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1693  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "608   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "\n",
       "[342 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>200329_1113_00-13-38.86--00-13-39.94.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.0186615   0.00944519  0.0038147  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>200309_2035_00-05-10.63--00-05-11.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-0.00149536 -0.00054932 -0.00024414 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>200229_2244_00-05-22.03--00-05-23.17.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.03457642 -0.03807068 -0.04052734 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>200306_2024_00-03-20.26--00-03-20.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.01483154  0.01954651  0.02474976 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>200306_2024_00-17-31.11--00-17-31.68.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.11328125  0.10844421  0.1025238  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>200815_2140_00-00-15.88--00-00-18.55.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00408936  0.0065918   0.01071167 ... -0.03...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>200307_1826_00-05-53.14--00-05-53.71.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00387573  0.0035553   0.00352478 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>200306_2024_00-08-21.67--00-08-24.98.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.01591492 -0.01504517 -0.01268005 ... -0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>200529_1029_00-01-10.01--00-01-11.35.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>210324_2036_00-06-12.32--00-06-13.43.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>social</td>\n",
       "      <td>[-0.00427246 -0.00418091 -0.00411987 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Filename Participant      Label  \\\n",
       "1186  200329_1113_00-13-38.86--00-13-39.94.wav         P01   selftalk   \n",
       "1626  200309_2035_00-05-10.63--00-05-11.91.wav         P01  delighted   \n",
       "1059  200229_2244_00-05-22.03--00-05-23.17.wav         P01   selftalk   \n",
       "866   200306_2024_00-03-20.26--00-03-20.91.wav         P01   selftalk   \n",
       "990   200306_2024_00-17-31.11--00-17-31.68.wav         P01   selftalk   \n",
       "...                                        ...         ...        ...   \n",
       "1677  200815_2140_00-00-15.88--00-00-18.55.wav         P01  delighted   \n",
       "1707  200307_1826_00-05-53.14--00-05-53.71.wav         P01  delighted   \n",
       "1194  200306_2024_00-08-21.67--00-08-24.98.wav         P01   selftalk   \n",
       "1693  200529_1029_00-01-10.01--00-01-11.35.wav         P01  delighted   \n",
       "608   210324_2036_00-06-12.32--00-06-13.43.wav         P01     social   \n",
       "\n",
       "                                                  Audio  delighted  \\\n",
       "1186  [ 0.0186615   0.00944519  0.0038147  ... -0.01...      False   \n",
       "1626  [-0.00149536 -0.00054932 -0.00024414 ... -0.00...       True   \n",
       "1059  [-0.03457642 -0.03807068 -0.04052734 ...  0.00...      False   \n",
       "866   [ 0.01483154  0.01954651  0.02474976 ...  0.00...      False   \n",
       "990   [ 0.11328125  0.10844421  0.1025238  ... -0.01...      False   \n",
       "...                                                 ...        ...   \n",
       "1677  [ 0.00408936  0.0065918   0.01071167 ... -0.03...       True   \n",
       "1707  [ 0.00387573  0.0035553   0.00352478 ... -0.00...       True   \n",
       "1194  [-0.01591492 -0.01504517 -0.01268005 ... -0.00...      False   \n",
       "1693  [-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...       True   \n",
       "608   [-0.00427246 -0.00418091 -0.00411987 ...  0.00...      False   \n",
       "\n",
       "      dysregulated  frustrated  request  selftalk  social  \n",
       "1186         False       False    False      True   False  \n",
       "1626         False       False    False     False   False  \n",
       "1059         False       False    False      True   False  \n",
       "866          False       False    False      True   False  \n",
       "990          False       False    False      True   False  \n",
       "...            ...         ...      ...       ...     ...  \n",
       "1677         False       False    False     False   False  \n",
       "1707         False       False    False     False   False  \n",
       "1194         False       False    False      True   False  \n",
       "1693         False       False    False     False   False  \n",
       "608          False       False    False     False    True  \n",
       "\n",
       "[342 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27919/387192210.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
      "/tmp/ipykernel_27919/387192210.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Create the dataloaders\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Create the model, criterion, optimizer and scheduler\u001b[39;00m\n\u001b[1;32m    153\u001b[0m model \u001b[38;5;241m=\u001b[39m AudioClassifier(n_classes)\n",
      "Cell \u001b[0;32mIn[23], line 126\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[0;34m(X_train, X_test, y_train, y_test, batch_size)\u001b[0m\n\u001b[1;32m    123\u001b[0m test_mels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([convert_to_mono(torch\u001b[38;5;241m.\u001b[39mtensor(ms)) \u001b[38;5;28;01mfor\u001b[39;00m ms \u001b[38;5;129;01min\u001b[39;00m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMel Spectrogram\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Convert one-hot encoded labels to class indices\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    127\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create TensorDataset\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Define the audio classification model\n",
    "\n",
    "class AudioClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super(AudioClassifier, self).__init__() # Initialize the parent class\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 1 input channel, 32 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 32 input channels, 64 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 64 input channels, 128 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 128 input channels, 256 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0) # Reduce the size of the input by half in both dimensions using max pooling with a 2x2 kernel window and stride\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256 * (n_mels // 16), 512) # fully connected layer: takes in the flattened output of the last convolutional layer and outputs 512 features with dimensions reduced by a factor of 16\n",
    "        self.fc2 = torch.nn.Linear(512, n_classes) # fully connected layer: takes in the 512 features from the previous layer and outputs the number of classes\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5) # Dropout layer with a dropout rate of 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # Apply ReLU activation function to the output of the first convolutional layer\n",
    "        x = self.pool(x) # Apply max pooling to the output of the first convolutional layer\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) # Repeat the same process for the next convolutional layer\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten the output of the last convolutional layer\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # Apply ReLU activation function to the output of the first fully connected layer\n",
    "        x = self.dropout(x) # Apply dropout to the output of the first fully connected layer to prevent overfitting\n",
    "        \n",
    "        x = self.fc2(x) # Apply the second fully connected layer to get the final output\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Define the function to train the model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device) # Move the data to the device\n",
    "            \n",
    "            optimizer.zero_grad() # Zero the gradients thereby preventing them from accumulating\n",
    "            \n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "            \n",
    "            loss = criterion(y_pred, y) # Calculate the loss\n",
    "            loss.backward() # Backpropagate the loss\n",
    "            optimizer.step() # Update the weights\n",
    "            \n",
    "            train_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "            \n",
    "        train_loss /= len(train_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "        train_acc /= len(train_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "        \n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "            for i, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "                \n",
    "                loss = criterion(y_pred, y) # Calculate the loss\n",
    "                \n",
    "                val_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "                \n",
    "            val_loss /= len(val_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "            val_acc /= len(val_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "            \n",
    "        print(\"Epoch: {} | Train Loss: {:.5f} | Train Acc: {:.5f} | Val Loss: {:.5f} | Val Acc: {:.5f}\".format(epoch, train_loss, train_acc, val_loss, val_acc)) # Print the epoch number, training loss, training accuracy, validation loss and validation accuracy\n",
    "        \n",
    "        scheduler.step(val_loss) # Adjust the learning rate based on the validation loss\n",
    "        \n",
    "# Define the function to test the model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode because certain layers like dropout behave differently in training and evaluation mode\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "            \n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "            \n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        \n",
    "    print(\"Test Accuracy: {:.5f}\".format(test_acc))\n",
    "    \n",
    "# Define the function to create the dataloaders\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     train_dataset = torch.utils.data.TensorDataset(X_train['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_train.values).long())\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     val_dataset = torch.utils.data.TensorDataset(X_test['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_test.values).long())\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "\n",
    "def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "    # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "    train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "    test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    train_labels = torch.argmax(torch.tensor(y_train.values), dim=1).long()\n",
    "    test_labels = torch.argmax(torch.tensor(y_test.values), dim=1).long()\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "\n",
    "n_classes = len(p01_df['Label'].unique())\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader, val_loader = create_dataloaders(X_train, X_test, y_train, y_test, batch_size)\n",
    "\n",
    "# Create the model, criterion, optimizer and scheduler\n",
    "\n",
    "model = AudioClassifier(n_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "test_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename        object\n",
      "Participant     object\n",
      "Label           object\n",
      "Audio           object\n",
      "delighted         bool\n",
      "dysregulated      bool\n",
      "frustrated        bool\n",
      "request           bool\n",
      "selftalk          bool\n",
      "social            bool\n",
      "dtype: object\n",
      "                                        Filename Participant         Label  \\\n",
      "348     200124_1828_00-09-14.62--00-09-15.67.wav         P01  dysregulated   \n",
      "85    200229_2156_00-08-01.665--00-08-03.284.wav         P01    frustrated   \n",
      "1526    200309_2035_00-01-53.76--00-01-54.45.wav         P01     delighted   \n",
      "393    200124_1828_00-01-01.464--00-01-02.72.wav         P01  dysregulated   \n",
      "1080     200306_2024_00-02-28.21--00-02-29.9.wav         P01      selftalk   \n",
      "\n",
      "                                                  Audio  delighted  \\\n",
      "348   [-0.05300903 -0.06085205 -0.06741333 ... -0.22...      False   \n",
      "85    [-0.0071106  -0.00773621 -0.00802612 ... -0.00...      False   \n",
      "1526  [-5.6457520e-03 -5.6152344e-03 -5.5236816e-03 ...       True   \n",
      "393   [ 0.13546753  0.18415833  0.2346344  ... -0.11...      False   \n",
      "1080  [-0.00106812 -0.00361633 -0.00590515 ... -0.00...      False   \n",
      "\n",
      "      dysregulated  frustrated  request  selftalk  social  \n",
      "348           True       False    False     False   False  \n",
      "85           False        True    False     False   False  \n",
      "1526         False       False    False     False   False  \n",
      "393           True       False    False     False   False  \n",
      "1080         False       False    False      True   False  \n"
     ]
    }
   ],
   "source": [
    "print(y_train.dtypes)  # Check the data types of the columns in y_train\n",
    "print(y_train.head())  # Display the first few rows of y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the index of the label\n",
    "def get_label_index(row):\n",
    "    labels = ['delighted', 'dysregulated', 'frustrated', 'request', 'selftalk', 'social']\n",
    "    for idx, label in enumerate(labels):\n",
    "        if row[label]:  # If the label is True, return its index\n",
    "            return idx\n",
    "    return -1  # If no label is True, return an invalid index (this shouldn't happen in a clean dataset)\n",
    "\n",
    "# Apply the function to create a label index for each row\n",
    "y_train['Label_Index'] = y_train.apply(get_label_index, axis=1)\n",
    "y_test['Label_Index'] = y_test.apply(get_label_index, axis=1)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "train_labels = torch.tensor(y_train['Label_Index'].values).long()\n",
    "test_labels = torch.tensor(y_test['Label_Index'].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "    # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "    train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "    test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "    \n",
    "    # Convert labels to single label index\n",
    "    y_train['Label_Index'] = y_train.apply(get_label_index, axis=1)\n",
    "    y_test['Label_Index'] = y_test.apply(get_label_index, axis=1)\n",
    "    \n",
    "    train_labels = torch.tensor(y_train['Label_Index'].values).long()\n",
    "    test_labels = torch.tensor(y_test['Label_Index'].values).long()\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27919/3568078184.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27919/3568078184.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m model \u001b[38;5;241m=\u001b[39m AudioClassifier(n_classes)\n\u001b[1;32m    154\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> 155\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/adam.py:73\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     62\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     63\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    364\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_compile.py:26\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:25\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdeprecated_func\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_symbolic_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fx_tracing\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/deprecated.py:113\u001b[0m\n\u001b[1;32m    111\u001b[0m setup_docs(vmap, apis\u001b[38;5;241m.\u001b[39mvmap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.vmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m setup_docs(grad, apis\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m--> 113\u001b[0m \u001b[43msetup_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_and_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m setup_docs(vjp)\n\u001b[1;32m    115\u001b[0m setup_docs(jvp)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/deprecated.py:44\u001b[0m, in \u001b[0;36msetup_docs\u001b[0;34m(functorch_api, torch_func_api, new_api_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m api_name \u001b[38;5;241m=\u001b[39m functorch_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     torch_func_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# See https://docs.python.org/3/using/cmdline.html#cmdoption-OO\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'"
     ]
    }
   ],
   "source": [
    "# Define the audio classification model\n",
    "\n",
    "class AudioClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super(AudioClassifier, self).__init__() # Initialize the parent class\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 1 input channel, 32 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 32 input channels, 64 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 64 input channels, 128 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 128 input channels, 256 output channels, 3x3 kernel, 1x1 stride, 1x1 padding\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0) # Reduce the size of the input by half in both dimensions using max pooling with a 2x2 kernel window and stride\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256 * (n_mels // 16), 512) # fully connected layer: takes in the flattened output of the last convolutional layer and outputs 512 features with dimensions reduced by a factor of 16\n",
    "        self.fc2 = torch.nn.Linear(512, n_classes) # fully connected layer: takes in the 512 features from the previous layer and outputs the number of classes\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5) # Dropout layer with a dropout rate of 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # Apply ReLU activation function to the output of the first convolutional layer\n",
    "        x = self.pool(x) # Apply max pooling to the output of the first convolutional layer\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) # Repeat the same process for the next convolutional layer\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten the output of the last convolutional layer\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # Apply ReLU activation function to the output of the first fully connected layer\n",
    "        x = self.dropout(x) # Apply dropout to the output of the first fully connected layer to prevent overfitting\n",
    "        \n",
    "        x = self.fc2(x) # Apply the second fully connected layer to get the final output\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Define the function to train the model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device) # Move the data to the device\n",
    "            \n",
    "            optimizer.zero_grad() # Zero the gradients thereby preventing them from accumulating\n",
    "            \n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "            \n",
    "            loss = criterion(y_pred, y) # Calculate the loss\n",
    "            loss.backward() # Backpropagate the loss\n",
    "            optimizer.step() # Update the weights\n",
    "            \n",
    "            train_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "            \n",
    "        train_loss /= len(train_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "        train_acc /= len(train_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "        \n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "            for i, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "                \n",
    "                loss = criterion(y_pred, y) # Calculate the loss\n",
    "                \n",
    "                val_loss += loss.item() # Add the loss to the running total thereby accumulating the loss\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "                \n",
    "            val_loss /= len(val_loader.dataset) # Calculate the average loss over the dataset for the epoch\n",
    "            val_acc /= len(val_loader.dataset) # Calculate the average accuracy over the dataset for the epoch\n",
    "            \n",
    "        print(\"Epoch: {} | Train Loss: {:.5f} | Train Acc: {:.5f} | Val Loss: {:.5f} | Val Acc: {:.5f}\".format(epoch, train_loss, train_acc, val_loss, val_acc)) # Print the epoch number, training loss, training accuracy, validation loss and validation accuracy\n",
    "        \n",
    "        scheduler.step(val_loss) # Adjust the learning rate based on the validation loss\n",
    "        \n",
    "# Define the function to test the model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode because certain layers like dropout behave differently in training and evaluation mode\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient tracking to speed up computation\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X) # Get the model's predictions by forward propagating the input\n",
    "            \n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item() # Calculate the accuracy by comparing the model's predictions to the actual labels and accumulating the correct predictions\n",
    "            \n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        \n",
    "    print(\"Test Accuracy: {:.5f}\".format(test_acc))\n",
    "    \n",
    "# Define the function to create the dataloaders\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     train_dataset = torch.utils.data.TensorDataset(X_train['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_train.values).long())\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     val_dataset = torch.utils.data.TensorDataset(X_test['Mel Spectrogram'].unsqueeze(1), torch.tensor(y_test.values).long())\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "\n",
    "# def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "#     # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "#     train_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_train['Mel Spectrogram']])\n",
    "#     test_mels = torch.stack([convert_to_mono(torch.tensor(ms)) for ms in X_test['Mel Spectrogram']])\n",
    "    \n",
    "#     # Convert one-hot encoded labels to class indices\n",
    "#     train_labels = torch.argmax(torch.tensor(y_train.values), dim=1).long()\n",
    "#     test_labels = torch.argmax(torch.tensor(y_test.values), dim=1).long()\n",
    "    \n",
    "#     # Create TensorDataset\n",
    "#     train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "#     val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "\n",
    "n_classes = len(p01_df['Label'].unique())\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader, val_loader = create_dataloaders(X_train, X_test, y_train, y_test, batch_size)\n",
    "\n",
    "# Create the model, criterion, optimizer and scheduler\n",
    "\n",
    "model = AudioClassifier(n_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "test_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.core import S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\n",
      "Uninstalling torch-2.4.0:\n",
      "  Successfully uninstalled torch-2.4.0\n",
      "Found existing installation: torchvision 0.19.0\n",
      "Uninstalling torchvision-0.19.0:\n",
      "  Successfully uninstalled torchvision-0.19.0\n",
      "Found existing installation: sympy 1.13.2\n",
      "Uninstalling sympy-1.13.2:\n",
      "  Successfully uninstalled sympy-1.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision sympy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.1\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.17.1+cu121 (from versions: 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.17.1+cu121\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1 torchvision==0.17.1+cu121 sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.1\n",
      "  Using cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.17.1\n",
      "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (4.11.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2.1)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.17.1) (10.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1) (12.5.82)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
      "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m179.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: triton, sympy, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lightning 2.3.3 requires PyYAML<8.0,>=5.4, which is not installed.\n",
      "pytorch-lightning 2.3.3 requires PyYAML>=5.4, which is not installed.\n",
      "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 sympy-1.13.2 torch-2.2.1 torchvision-0.17.1 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1 torchvision==0.17.1 sympy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
