{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned HuBERT model\n",
    "This notebook trains a model to predict the label of a vocalization by individual `P01`. Labels with fewer than 30 examples in the training data are discarded. Remaining labels:\n",
    " - `selftalk` (444 examples)\n",
    " - `delighted` (287 examples)\n",
    " - `dysregulated` (179 examples)\n",
    " - `social` (143 examples)\n",
    " - `frustrated` (118 examples)\n",
    " - `request` (107 examples)\n",
    " - `dysregulation-sick` (59 examples)\n",
    "\n",
    " The model uses features generated by a pre-trained HuBERT model, which are fed into a regularized multi-class logistic regression. Out-of-sample performance:\n",
    " - accuracy: 0.757\n",
    " - F1 score (unweighted): 0.779\n",
    " - cross-entropy: 0.689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, log_loss\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_predict,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained HuBERT model\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "model = bundle.get_model()\n",
    "\n",
    "# List of data files\n",
    "data_files = pd.read_csv(\"../data/directory_w_train_test.csv\")\n",
    "data_files_p1 = data_files.loc[data_files.Participant == \"P01\"]\n",
    "label_counts = data_files_p1.Label.value_counts()\n",
    "training_files = data_files_p1.loc[\n",
    "    data_files_p1.Label.isin(label_counts[label_counts >= 30].index)\n",
    "    & (data_files_p1.is_test == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features)=<class 'list'>, len(features)=12\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Extract acoustic features from one wav file using\n",
    "# the pretrained\n",
    "datadir = Path(\"../data/wav\")\n",
    "filename = training_files.Filename.iloc[0]\n",
    "waveform, sample_rate = torchaudio.load(datadir / filename)\n",
    "waveform = torchaudio.functional.resample(\n",
    "    waveform, sample_rate, bundle.sample_rate\n",
    ")\n",
    "features, _ = model.extract_features(waveform)\n",
    "\n",
    "# features is a list of 12 tensors, each having shape\n",
    "# (m, n, 768), where the value of m and n are different\n",
    "# depending on the audio sample (m is always 1 or 2, n\n",
    "# varies more widely and I think depends on the length\n",
    "# of the clip). I'm averaging over time (n).\n",
    "print(f\"{type(features)=}, {len(features)=}\")\n",
    "print(features[0].mean((0, 1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8486ffb4ecc47aeb2f0c3b695855624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1337, 768]) torch.Size([1337])\n"
     ]
    }
   ],
   "source": [
    "# Generate features using the pretrained model.\n",
    "# We will use only the first layer of generated features.\n",
    "with torch.no_grad():\n",
    "    t_list = []\n",
    "    for filename in tqdm(training_files.Filename):\n",
    "        waveform, sample_rate = torchaudio.load(datadir / filename)\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, bundle.sample_rate\n",
    "        )\n",
    "\n",
    "        features, _ = model.extract_features(waveform)\n",
    "        t_list.append(features[0].mean((0, 1)))\n",
    "\n",
    "X = torch.stack(t_list).detach()\n",
    "labels = training_files.Label.unique()\n",
    "y = torch.zeros(len(training_files), dtype=torch.int)\n",
    "for idx, label in enumerate(labels):\n",
    "    y[(training_files.Label == label).values] = idx\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('logisticregression__C', 0.047272594888604615)])\n",
      "Best accuracy: 0.7599259342385816\n"
     ]
    }
   ],
   "source": [
    "# There are 768 generated features, which is a lot\n",
    "# relative to how many training data there are. So we\n",
    "# will need regularization. Using sk-optimize to optimize\n",
    "# strength of regularization parameter (this is overkill\n",
    "# since there's just one parameter, but oh well)\n",
    "est = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        max_iter=10**6,\n",
    "    ),\n",
    ")\n",
    "opt = BayesSearchCV(\n",
    "    est,\n",
    "    {\n",
    "        \"logisticregression__C\": (5e-3, 1, \"log-uniform\"),\n",
    "    },\n",
    "    n_iter=20,\n",
    "    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=12345),\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "opt.fit(\n",
    "    X.reshape(len(X), -1),\n",
    "    y,\n",
    ")\n",
    "print(opt.best_params_)\n",
    "print(\"Best accuracy:\", opt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oos_pred_prob.shape=(1337, 7), oos_pred.shape=(1337,)\n",
      "Accuracy: 0.758\n",
      "Unweighted F1 score: 0.781\n",
      "Cross-entropy: 0.691\n"
     ]
    }
   ],
   "source": [
    "# Generate out-of-sample predictions using a logistic\n",
    "# regression model, with the parameter determined by\n",
    "# the optimization above.\n",
    "est = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        C=opt.best_params_[\"logisticregression__C\"],\n",
    "        max_iter=10**6,\n",
    "    ),\n",
    ")\n",
    "oos_pred_prob = cross_val_predict(\n",
    "    est,\n",
    "    X.reshape(len(X), -1),\n",
    "    y,\n",
    "    cv=StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=1234,  # Using different seed to avoid over-fitting parameter\n",
    "    ),\n",
    "    method=\"predict_proba\",\n",
    ")\n",
    "oos_pred = oos_pred_prob.argmax(1)\n",
    "print(f\"{oos_pred_prob.shape=}, {oos_pred.shape=}\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y, oos_pred):.3f}\")\n",
    "print(f\"Unweighted F1 score: {f1_score(y, oos_pred, average='macro'):.3f}\")\n",
    "print(f\"Cross-entropy: {log_loss(y, oos_pred_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred_label</th>\n",
       "      <th>dysregulation-sick</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>social</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>request</th>\n",
       "      <th>delighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dysregulation-sick</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dysregulated</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftalk</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>341</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delighted</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred_label          dysregulation-sick  frustrated  dysregulated  social  \\\n",
       "actual_label                                                               \n",
       "dysregulation-sick                  50           2             1       0   \n",
       "frustrated                           1          82             2       2   \n",
       "dysregulated                         1           4           160       1   \n",
       "social                               0           2             1     112   \n",
       "selftalk                             6           6             2      15   \n",
       "request                              0           0             0       0   \n",
       "delighted                            3           7             1       9   \n",
       "\n",
       "pred_label          selftalk  request  delighted  \n",
       "actual_label                                      \n",
       "dysregulation-sick         3        1          2  \n",
       "frustrated                12        0         19  \n",
       "dysregulated               9        0          4  \n",
       "social                    11        0         17  \n",
       "selftalk                 341       17         57  \n",
       "request                   12       84         11  \n",
       "delighted                 80        3        184  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "# selftalk and delighted are frequently confused by this model\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    confusion_matrix(y, oos_pred), columns=labels, index=labels\n",
    ")\n",
    "conf_matrix_df.index.name = \"actual_label\"\n",
    "conf_matrix_df.columns.name = \"pred_label\"\n",
    "display(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HuBERT features are outputs of an early layer in the HuBERT model. Fitting a logistic regression to these outputs can be interpretted as truncating HuBERT at an appropriate layer, appending a linear layer and a softmax layer, then training the new model keeping the HuBERT parameters frozen. A possible approach to improve this model could be to selective unfreezely HuBERT layers at some points during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
