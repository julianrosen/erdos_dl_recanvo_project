{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned HuBERT model\n",
    "This notebook trains a model to predict the label of a vocalization by individual `P05`. Labels with fewer than 30 examples in the training data are discarded. Remaining labels:\n",
    " - `selftalk` (231 examples)\n",
    " - `frustrated` (223 examples)\n",
    " - `delighted` (182 examples)\n",
    " - `dysregulated` (92 examples) \n",
    " - `happy` (49 examples)\n",
    "\n",
    "\n",
    "I testing using ordinary 10-fold CV, and also leave-one-session-out CV. Performance is significantly better with 10-fold CV, which indicates that the model is picking up on things specific to sessions (perhaps background noise). I expect leave-one-session-out CV is a more accurate estimation of the generalizability of a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_predict,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def to_prob(metric):\n",
    "    @functools.wraps(metric)\n",
    "    def metric_that_takes_prob(y_actual, y_pred, sample_weight=None):\n",
    "        return metric(y_actual, y_pred.argmax(1), sample_weight=sample_weight)\n",
    "\n",
    "    return metric_that_takes_prob\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": to_prob(accuracy_score),\n",
    "    \"balanced_accuracy\": to_prob(balanced_accuracy_score),\n",
    "    \"unweighted_f1\": to_prob(functools.partial(f1_score, average=\"macro\")),\n",
    "    \"UAR\": to_prob(functools.partial(recall_score, average=\"macro\")),\n",
    "    \"logloss\": log_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "selftalk        231\n",
       "frustrated      223\n",
       "delighted       182\n",
       "dysregulated     92\n",
       "happy            49\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pretrained HuBERT model\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "model = bundle.get_model()\n",
    "\n",
    "# List of data files\n",
    "data_files = pd.read_csv(\"../data/directory_w_train_test.csv\")\n",
    "data_files_p5 = data_files.loc[data_files.Participant == \"P05\"]\n",
    "label_counts = data_files_p5.Label.value_counts()\n",
    "training_files = data_files_p5.loc[\n",
    "    data_files_p5.Label.isin(label_counts[label_counts >= 30].index)\n",
    "    & (data_files_p5.is_test == 0)\n",
    "].copy()\n",
    "training_files[\"session\"] = training_files.Filename.apply(\n",
    "    lambda name: name.split(\"-\")[0][:-3]\n",
    ")\n",
    "display(training_files.Label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features)=<class 'list'>, len(features)=12\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Extract acoustic features from one wav file using\n",
    "# the pretrained\n",
    "\n",
    "datadir = Path(\"../data/wav\")\n",
    "filename = training_files.Filename.iloc[0]\n",
    "waveform, sample_rate = torchaudio.load(datadir / filename)\n",
    "waveform = torchaudio.functional.resample(\n",
    "    waveform, sample_rate, bundle.sample_rate\n",
    ")\n",
    "features, _ = model.extract_features(waveform)\n",
    "\n",
    "# features is a list of 12 tensors, each having shape\n",
    "# (m, n, 768), where the value of m and n are different\n",
    "# depending on the audio sample (m is always 1 or 2, n\n",
    "# varies more widely and I think depends on the length\n",
    "# of the clip). I'm averaging over time (n).\n",
    "print(f\"{type(features)=}, {len(features)=}\")\n",
    "print(features[0].mean((0, 1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745455dfef1b45a3b0838f270d44cf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/777 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([777, 768]) torch.Size([777])\n"
     ]
    }
   ],
   "source": [
    "# Generate features using the pretrained model.\n",
    "# We will use only the first layer of generated features.\n",
    "with torch.no_grad():\n",
    "    t_list = []\n",
    "    for filename in tqdm(training_files.Filename):\n",
    "        waveform, sample_rate = torchaudio.load(datadir / filename)\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, bundle.sample_rate\n",
    "        )\n",
    "\n",
    "        features, _ = model.extract_features(waveform)\n",
    "        t_list.append(features[0].mean((0, 1)))\n",
    "\n",
    "X = torch.stack(t_list).detach()\n",
    "labels = training_files.Label.unique()\n",
    "y = torch.zeros(len(training_files), dtype=torch.int)\n",
    "for idx, label in enumerate(labels):\n",
    "    y[(training_files.Label == label).values] = idx\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('logisticregression__C', 0.01331684278118987)])\n",
      "Best accuracy: 0.6654345654345655\n"
     ]
    }
   ],
   "source": [
    "# There are 768 generated features, which is a lot\n",
    "# relative to how many training data there are. So we\n",
    "# will need regularization. Using sk-optimize to optimize\n",
    "# strength of regularization parameter (this is overkill\n",
    "# since there's just one parameter, but oh well)\n",
    "est = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        max_iter=10**6,\n",
    "    ),\n",
    ")\n",
    "opt = BayesSearchCV(\n",
    "    est,\n",
    "    {\n",
    "        \"logisticregression__C\": (5e-3, 1, \"log-uniform\"),\n",
    "    },\n",
    "    n_iter=20,\n",
    "    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=12345),\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "opt.fit(\n",
    "    X.reshape(len(X), -1),\n",
    "    y,\n",
    ")\n",
    "print(opt.best_params_)\n",
    "print(\"Best accuracy:\", opt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some possible sample weights for training and/or metrics.\n",
    "# session_weight weights each sample based on sessions, so\n",
    "# that the total weight of observations in each sesion is\n",
    "# constant. On top of that, session_and_label_weight assigns\n",
    "# a label to each weight, which is multiplied by the sesion weight,\n",
    "# in such a way to make the sum of the weights constant by label.\n",
    "session_weight = (\n",
    "    (1 / training_files.session.value_counts())\n",
    "    .clip(None, 0.1)\n",
    "    .loc[training_files.session]\n",
    ").values\n",
    "session_and_label_weight = (\n",
    "    1\n",
    "    / pd.Series(session_weight, training_files.index)\n",
    "    .groupby(training_files.Label)\n",
    "    .sum()\n",
    ").loc[training_files.Label].values * session_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy             0.614\n",
       "balanced_accuracy    0.511\n",
       "unweighted_f1        0.525\n",
       "UAR                  0.511\n",
       "logloss              1.066\n",
       "Name: no_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "accuracy             0.645\n",
       "balanced_accuracy    0.528\n",
       "unweighted_f1        0.549\n",
       "UAR                  0.528\n",
       "logloss              1.028\n",
       "Name: session_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "accuracy             0.528\n",
       "balanced_accuracy    0.528\n",
       "unweighted_f1        0.496\n",
       "UAR                  0.528\n",
       "logloss              1.239\n",
       "Name: session_and_label_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate out-of-sample predictions using a logistic\n",
    "# regression model, with the parameter determined by\n",
    "# the optimization above.\n",
    "#\n",
    "# We compute various metrics, using various weightings\n",
    "est = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        C=opt.best_params_[\"logisticregression__C\"],\n",
    "        max_iter=10**6,\n",
    "    ),\n",
    ")\n",
    "oos_pred_prob = cross_val_predict(\n",
    "    est,\n",
    "    X.reshape(len(X), -1),\n",
    "    y,\n",
    "    cv=StratifiedKFold(\n",
    "        n_splits=10,\n",
    "        shuffle=True,\n",
    "        random_state=1234,  # Using different seed to avoid over-fitting parameter\n",
    "    ),\n",
    "    method=\"predict_proba\",\n",
    "    params={\"logisticregression__sample_weight\": session_weight},\n",
    ")\n",
    "oos_pred = oos_pred_prob.argmax(1)\n",
    "\n",
    "display(\n",
    "    pd.Series(\n",
    "        {name: metric(y, oos_pred_prob) for name, metric in metrics.items()},\n",
    "        name=\"no_weight\",\n",
    "    ).round(3)\n",
    ")\n",
    "display(\n",
    "    pd.Series(\n",
    "        {\n",
    "            name: metric(y, oos_pred_prob, sample_weight=session_weight)\n",
    "            for name, metric in metrics.items()\n",
    "        },\n",
    "        name=\"session_weight\",\n",
    "    ).round(3)\n",
    ")\n",
    "display(\n",
    "    pd.Series(\n",
    "        {\n",
    "            name: metric(\n",
    "                y, oos_pred_prob, sample_weight=session_and_label_weight\n",
    "            )\n",
    "            for name, metric in metrics.items()\n",
    "        },\n",
    "        name=\"session_and_label_weight\",\n",
    "    ).round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred_label</th>\n",
       "      <th>happy</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>delighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frustrated</th>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dysregulated</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftalk</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delighted</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred_label    happy  frustrated  dysregulated  selftalk  delighted\n",
       "actual_label                                                      \n",
       "happy            17           1             1        15         15\n",
       "frustrated        0         163             5        19         36\n",
       "dysregulated      4          32            11        33         12\n",
       "selftalk          0           7             0       185         39\n",
       "delighted         0          10             2        69        101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "# selftalk and delighted are frequently confused by this model\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    confusion_matrix(y, oos_pred), columns=labels, index=labels\n",
    ")\n",
    "conf_matrix_df.index.name = \"actual_label\"\n",
    "conf_matrix_df.columns.name = \"pred_label\"\n",
    "display(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy             0.524\n",
       "balanced_accuracy    0.445\n",
       "unweighted_f1        0.456\n",
       "UAR                  0.445\n",
       "logloss              1.259\n",
       "Name: no_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy             0.526\n",
       "balanced_accuracy    0.458\n",
       "unweighted_f1        0.466\n",
       "UAR                  0.458\n",
       "logloss              1.206\n",
       "Name: session_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy             0.458\n",
       "balanced_accuracy    0.458\n",
       "unweighted_f1        0.444\n",
       "UAR                  0.458\n",
       "logloss              1.432\n",
       "Name: session_and_label_weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hold one session out CV\n",
    "# Performance is substantially worse, so the model\n",
    "# is probably picking up on background sounds. But\n",
    "# there is still some predictive power.\n",
    "\n",
    "# Here I'm running the CV for many different values\n",
    "# of the regularization parameter C, and logging\n",
    "# some different scores. Optimal value of C depends\n",
    "# on whether whether metrics weight classes by class\n",
    "# size.\n",
    "scores = []\n",
    "\n",
    "# C_list = [0.02 * 0.8**n for n in range(20)]\n",
    "# for C in tqdm(C_list):\n",
    "C = 0.007\n",
    "pred_prob = np.zeros((len(X), len(labels)))\n",
    "pred_prob[:] = np.nan\n",
    "for session in training_files.session.unique():\n",
    "    mask = (training_files.session == session).values\n",
    "    est = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            C=C,\n",
    "            max_iter=10**6,\n",
    "        ),\n",
    "    )\n",
    "    est.fit(\n",
    "        X[~mask],\n",
    "        y[~mask],\n",
    "    )\n",
    "    pred_prob[mask] = est.predict_proba(X[mask])\n",
    "assert not np.isnan(pred_prob).any()\n",
    "pred = pred_prob.argmax(1)\n",
    "display(\n",
    "    pd.Series(\n",
    "        {name: metric(y, pred_prob) for name, metric in metrics.items()},\n",
    "        name=\"no_weight\",\n",
    "    ).round(3)\n",
    ")\n",
    "print()\n",
    "display(\n",
    "    pd.Series(\n",
    "        {\n",
    "            name: metric(y, pred_prob, sample_weight=session_weight)\n",
    "            for name, metric in metrics.items()\n",
    "        },\n",
    "        name=\"session_weight\",\n",
    "    ).round(3)\n",
    ")\n",
    "print()\n",
    "display(\n",
    "    pd.Series(\n",
    "        {\n",
    "            name: metric(y, pred_prob, sample_weight=session_and_label_weight)\n",
    "            for name, metric in metrics.items()\n",
    "        },\n",
    "        name=\"session_and_label_weight\",\n",
    "    ).round(3)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
